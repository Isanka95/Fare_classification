{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "stacking model.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOX1HBNA6BwG",
        "colab_type": "text"
      },
      "source": [
        "## 1. Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el92AualhO86",
        "trusted": true,
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "252bc445-158d-4adc-919b-73eebcfce3b9"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "from google.colab import drive  \n",
        "drive.mount('/content/gdrive')\n",
        "train_set= pd.read_csv('/content/gdrive/My Drive/Ride_Fare/train.csv', index_col=\"tripid\")\n",
        "test_set= pd.read_csv('/content/gdrive/My Drive/Ride_Fare/test.csv', index_col=\"tripid\")    \n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiVCHOp7By5Z",
        "colab_type": "text"
      },
      "source": [
        "## 2. Feature PreProcessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2xyvQZ92y5ND",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "705ba6ae-aade-4aec-cf73-8c129161221c"
      },
      "source": [
        "#Label encoding\n",
        "train_set[\"label\"] = train_set[\"label\"].map({\"incorrect\": 0, \"correct\":1})\n",
        "\n",
        "#fill misding values of 'duration' column using 'pickup_time' and 'drop_time' values\n",
        "train_set[\"pickup\"] = pd.to_datetime(train_set[\"pickup_time\"])\n",
        "train_set[\"drop\"] = pd.to_datetime(train_set[\"drop_time\"])\n",
        "train_set['difference'] = (train_set[\"drop\"] - train_set[\"pickup\"])/np.timedelta64(1,'s')\n",
        "train_set['duration'] = train_set['duration'].fillna(train_set['difference'])\n",
        "\n",
        "# define new feature 'distance'\n",
        "\n",
        "def haversine_vectorize(lon1, lat1, lon2, lat2):\n",
        " \n",
        "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
        " \n",
        "    newlon = lon2 - lon1\n",
        "    newlat = lat2 - lat1\n",
        " \n",
        "    haver_formula = np.sin(newlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(newlon/2.0)**2\n",
        " \n",
        "    dist = 2 * np.arcsin(np.sqrt(haver_formula ))\n",
        "    km = 6367 * dist #6367 for distance in KM for miles use 3958\n",
        "    return km\n",
        "\n",
        "train_set['distance'] = haversine_vectorize(train_set['pick_lon'],train_set['pick_lat'],train_set['drop_lon'],train_set['drop_lat'])\n",
        "\n",
        "# dropping features\n",
        "train_set = train_set.drop(['pick_lat','pick_lon','drop_lat','drop_lon','pickup_time','drop_time','pickup','drop','difference','meter_waiting_till_pickup'],axis='columns')\n",
        "\n",
        "train_set.info()\n",
        "\n",
        "X = train_set.drop(['label'],axis='columns')\n",
        "Y = train_set['label']"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 17176 entries, 189123628 to 213817296\n",
            "Data columns (total 7 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   additional_fare     16974 non-null  float64\n",
            " 1   duration            17176 non-null  float64\n",
            " 2   meter_waiting       16974 non-null  float64\n",
            " 3   meter_waiting_fare  16974 non-null  float64\n",
            " 4   fare                17039 non-null  float64\n",
            " 5   label               17176 non-null  int64  \n",
            " 6   distance            17176 non-null  float64\n",
            "dtypes: float64(6), int64(1)\n",
            "memory usage: 1.0 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mnn4e6YRB-QC",
        "colab_type": "text"
      },
      "source": [
        "## 3. Model Creation and Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbUKeCI1hlpA",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numeric_cols = ['additional_fare', 'duration', 'meter_waiting', 'meter_waiting_fare', 'fare', 'distance']\n",
        "\n",
        "numeric_preprocessing_steps = Pipeline(steps = [\n",
        "    ('standard_scaler', StandardScaler()),\n",
        "    ('imputer', SimpleImputer(strategy='mean'))])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers = [\n",
        "        ('num', numeric_preprocessing_steps, numeric_cols)\n",
        "    ],\n",
        "    remainder = \"drop\"\n",
        ")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhQIawgsjtyS",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "# get the models to evaluate\n",
        "level0 = list()\n",
        "level0.append(('xg',XGBClassifier(n_estimators=500,subsample=0.14))) \n",
        "level0.append(('mlp',MLPClassifier(hidden_layer_sizes=(50,100,50), max_iter=1000)))\n",
        "level0.append(('dt', RandomForestClassifier(n_estimators = 100,max_features = 'log2')))\n",
        "\n",
        "# define meta learner model\n",
        "level1 = LogisticRegression(penalty=\"l2\", C=3)\n",
        " \n",
        "estimator = StackingClassifier(estimators=level0, final_estimator=level1, cv=10)\n",
        "\n",
        "fullPipe = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', estimator)])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stnR949Jk056",
        "trusted": true,
        "collapsed": true,
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4acadbbc-1fff-42b4-b263-ba36f3f61caa"
      },
      "source": [
        "X_train, X_eval, y_train, y_eval = train_test_split(X, Y, test_size=0.33, shuffle=True, stratify=Y, random_state=6)\n",
        "\n",
        "# Train model\n",
        "fullPipe.fit(X_train, np.ravel(y_train))\n",
        "\n",
        "preds = fullPipe.predict(X_eval)\n",
        "y_preds = pd.DataFrame({\"label\": preds},index = y_eval.index)\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "print(f1_score(y_eval, y_preds))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9750820938767626\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xTm0tBQlBKF",
        "trusted": true,
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "outputId": "2cd4685b-39a0-4c1b-e154-0045acf50039"
      },
      "source": [
        "fullPipe.fit(X, Y)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('preprocessor',\n",
              "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
              "                                   sparse_threshold=0.3,\n",
              "                                   transformer_weights=None,\n",
              "                                   transformers=[('num',\n",
              "                                                  Pipeline(memory=None,\n",
              "                                                           steps=[('standard_scaler',\n",
              "                                                                   StandardScaler(copy=True,\n",
              "                                                                                  with_mean=True,\n",
              "                                                                                  with_std=True)),\n",
              "                                                                  ('imputer',\n",
              "                                                                   SimpleImputer(add_indicator=False,\n",
              "                                                                                 copy=True,\n",
              "                                                                                 fill_value=None,\n",
              "                                                                                 missing_values=nan...\n",
              "                                                                        verbose=0,\n",
              "                                                                        warm_start=False))],\n",
              "                                    final_estimator=LogisticRegression(C=3,\n",
              "                                                                       class_weight=None,\n",
              "                                                                       dual=False,\n",
              "                                                                       fit_intercept=True,\n",
              "                                                                       intercept_scaling=1,\n",
              "                                                                       l1_ratio=None,\n",
              "                                                                       max_iter=100,\n",
              "                                                                       multi_class='auto',\n",
              "                                                                       n_jobs=None,\n",
              "                                                                       penalty='l2',\n",
              "                                                                       random_state=None,\n",
              "                                                                       solver='lbfgs',\n",
              "                                                                       tol=0.0001,\n",
              "                                                                       verbose=0,\n",
              "                                                                       warm_start=False),\n",
              "                                    n_jobs=None, passthrough=False,\n",
              "                                    stack_method='auto', verbose=0))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AumdiEIICKN8",
        "colab_type": "text"
      },
      "source": [
        "## 4. Test data PreProcessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuPktjVnlVDH",
        "trusted": true,
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "5a3f638b-28bc-429e-efaa-77e1d4bd0557"
      },
      "source": [
        "#check missing values(no missing values)\n",
        "test_set.isnull().sum()\n",
        "\n",
        "#define new feature 'distance'\n",
        "test_set['distance'] = haversine_vectorize(test_set['pick_lon'],test_set['pick_lat'],test_set['drop_lon'],test_set['drop_lat'])\n",
        "\n",
        "# dropping features\n",
        "test_set = test_set.drop(['pick_lat','pick_lon','drop_lat','drop_lon','pickup_time','drop_time','meter_waiting_till_pickup'],axis='columns')\n",
        "\n",
        "test_set.info()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 8576 entries, 213284604 to 222860703\n",
            "Data columns (total 6 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   additional_fare     8576 non-null   float64\n",
            " 1   duration            8576 non-null   int64  \n",
            " 2   meter_waiting       8576 non-null   int64  \n",
            " 3   meter_waiting_fare  8576 non-null   float64\n",
            " 4   fare                8576 non-null   float64\n",
            " 5   distance            8576 non-null   float64\n",
            "dtypes: float64(4), int64(2)\n",
            "memory usage: 469.0 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USoJlXJHnKJ6",
        "colab_type": "text"
      },
      "source": [
        "## 5. Make Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8AAFpga6osV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = fullPipe.predict(test_set)\n",
        "test_set['prediction'] = result\n",
        "\n",
        "submission_df = test_set.drop(['additional_fare','distance','duration','fare','meter_waiting','meter_waiting_fare'],axis='columns')\n",
        "\n",
        "submission_df.head()\n",
        "datapath = '/content/gdrive/My Drive/Ride_Fare/'\n",
        "submission_df.to_csv(datapath +'submission.csv', index=True)"
      ],
      "execution_count": 30,
      "outputs": []
    }
  ]
}